{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DOWNLOAD_URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "DATA_PATH = os.path.join(\"data\")\n",
    "\n",
    "def fetch_data(data_download_url = DATA_DOWNLOAD_URL, data_path = DATA_PATH):\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    tgz_path = os.path.join(data_path, \"aclImdb_v1.tar.gz\")\n",
    "    urllib.request.urlretrieve(data_download_url, tgz_path) \n",
    "    reviews_tgz = tarfile.open(tgz_path)\n",
    "    reviews_tgz.extractall(path=data_path)\n",
    "    reviews_tgz.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aclImdb_v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "tgz_path = os.path.join(os.path.join(\"data\"), \"aclImdb_v1.tar.gz\")\n",
    "print(tgz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:07\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyprind\n",
    "\n",
    "basepath = 'data/aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000) # Create a processing bar.\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s , l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyprind\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/30/e76fb0c45da8aef49ea8d2a90d4e7a6877b45894c25f12fb961f009a891e/PyPrind-2.11.2-py3-none-any.whl\n",
      "Installing collected packages: pyprind\n",
      "Successfully installed pyprind-2.11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorboard 1.8.0 has requirement bleach==1.5.0, but you'll have bleach 2.1.3 which is incompatible.\n",
      "tensorboard 1.8.0 has requirement html5lib==0.9999999, but you'll have html5lib 1.0.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This 1931 comedy gets better with every viewin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maybe it's because I'm no fan of the comics (b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This service comedy, for which Peter Marshall ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/10- 30 minutes of pure holiday terror. Okay,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This Drummond entry is lacking in continuity. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  This 1931 comedy gets better with every viewin...          1\n",
       "1  Maybe it's because I'm no fan of the comics (b...          0\n",
       "2  This service comedy, for which Peter Marshall ...          0\n",
       "3  9/10- 30 minutes of pure holiday terror. Okay,...          1\n",
       "4  This Drummond entry is lacking in continuity. ...          0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomize data-frame and save as a CSV file\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# randomize data frame\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "# sace data frame to a csv file called movie_data.csv\n",
    "df.to_csv('movie_data.csv', index = False, encoding = 'utf-8')\n",
    "\n",
    "## ===== Read CSV file and check frist 5 rows ====##\n",
    "df = pd.read_csv('movie_data.csv', encoding = 'utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This 1931 comedy gets better with every viewing because of the comedic talents of Marion Davies and a terrific performance by C. Aubrey Smith. Smith plays a gruff old man who gathers his grown children (from his younger days as a rake) in his declining years. One is American (Davies), one English (Ray Milland who looks about 18), and one Italian (Nina Quartero). There are some surprises as the plot moves along with Ralph Forbes(was has no appeal at all) falling for Davies.<br /><br />Davies and Smith are just wonderful together and very touching. Davies also gets to do a few dances and make a few \"big\" entrances. And of course Davies is just gorgeous.<br /><br />Halliwell Hobbes, Doris Lloyd, Elizabeth Murray, Guinn Williams, Edgar Norton, and David Torrence co-star. Had they given out supporting Oscar awards in 1931, Smith might well have been nominated. He\\'s just excellent in this this gem.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-999:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer1(text):\n",
    "    return text.split()\n",
    "tokenizer1('running like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, there’s a technique to generate words into their root form. This technique is called \"[word stemming](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\".  \"[Porter Stemmer](http://snowball.tartarus.org/algorithms/porter/stemmer.html)\" is quite popular among researchers in the NLP domain. In the below code segment you can see how we can use NLTK package’s PorterStemmer to obtain the root form of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "Requirement already satisfied: six in /home/aapfuser/.virtualenvs/dev-aaweb/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /home/aapfuser/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorboard 1.8.0 has requirement bleach==1.5.0, but you'll have bleach 2.1.3 which is incompatible.\n",
      "tensorboard 1.8.0 has requirement html5lib==0.9999999, but you'll have html5lib 1.0.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter('running like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another vital concept in the data cleaning and pre-processing step is the concept known as “**stop word removal**”. “stop words” are the words that are commonly occur in all forms of texts and probably bear no useful information. Few ‘stop words’ are, is, and, has, are, have, like… Stop word removal makes our text processing mechanism efficient as it reduces the number of words we need to analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'run', 'like', 'run', 'and', 'run', 'a', 'lot']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('a running likes running and runs a lot')[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aapfuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['run', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a running likes running and runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Version(sklearn_version) < '0.18':\n",
    "    clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "else:\n",
    "    clf = SGDClassifier(loss='log', random_state=1, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stream = stream_docs(path='movie_data.csv')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)  # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"This 1931 comedy gets better with every viewing because of the comedic talents of Marion Davies and a terrific performance by C. Aubrey Smith. Smith plays a gruff old man who gathers his grown children (from his younger days as a rake) in his declining years. One is American (Davies), one English (Ray Milland who looks about 18), and one Italian (Nina Quartero). There are some surprises as the plot moves along with Ralph Forbes(was has no appeal at all) falling for Davies.<br /><br />Davies and Smith are just wonderful together and very touching. Davies also gets to do a few dances and make a few \"\"big\"\" entrances. And of course Davies is just gorgeous.<br /><br />Halliwell Hobbes, Doris Lloyd, Elizabeth Murray, Guinn Williams, Edgar Norton, and David Torrence co-star. Had they given out supporting Oscar awards in 1931, Smith might well have been nominated. He\\'s just excellent in this this gem.\"',\n",
       " 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs('movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size): \n",
    "    # size is the minibath put from doc_stream\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list, label_list = get_minibatch(stream_docs('movie_data.csv'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1931',\n",
       " 'comedy',\n",
       " 'gets',\n",
       " 'better',\n",
       " 'every',\n",
       " 'viewing',\n",
       " 'comedic',\n",
       " 'talents',\n",
       " 'marion',\n",
       " 'davies',\n",
       " 'terrific',\n",
       " 'performance',\n",
       " 'c',\n",
       " 'aubrey',\n",
       " 'smith',\n",
       " 'smith',\n",
       " 'plays',\n",
       " 'gruff',\n",
       " 'old',\n",
       " 'man',\n",
       " 'gathers',\n",
       " 'grown',\n",
       " 'children',\n",
       " 'younger',\n",
       " 'days',\n",
       " 'rake',\n",
       " 'declining',\n",
       " 'years',\n",
       " 'one',\n",
       " 'american',\n",
       " 'davies',\n",
       " 'one',\n",
       " 'english',\n",
       " 'ray',\n",
       " 'milland',\n",
       " 'looks',\n",
       " '18',\n",
       " 'one',\n",
       " 'italian',\n",
       " 'nina',\n",
       " 'quartero',\n",
       " 'surprises',\n",
       " 'plot',\n",
       " 'moves',\n",
       " 'along',\n",
       " 'ralph',\n",
       " 'forbes',\n",
       " 'appeal',\n",
       " 'falling',\n",
       " 'davies',\n",
       " 'davies',\n",
       " 'smith',\n",
       " 'wonderful',\n",
       " 'together',\n",
       " 'touching',\n",
       " 'davies',\n",
       " 'also',\n",
       " 'gets',\n",
       " 'dances',\n",
       " 'make',\n",
       " 'big',\n",
       " 'entrances',\n",
       " 'course',\n",
       " 'davies',\n",
       " 'gorgeous',\n",
       " 'halliwell',\n",
       " 'hobbes',\n",
       " 'doris',\n",
       " 'lloyd',\n",
       " 'elizabeth',\n",
       " 'murray',\n",
       " 'guinn',\n",
       " 'williams',\n",
       " 'edgar',\n",
       " 'norton',\n",
       " 'david',\n",
       " 'torrence',\n",
       " 'co',\n",
       " 'star',\n",
       " 'given',\n",
       " 'supporting',\n",
       " 'oscar',\n",
       " 'awards',\n",
       " '1931',\n",
       " 'smith',\n",
       " 'might',\n",
       " 'well',\n",
       " 'nominated',\n",
       " 'excellent',\n",
       " 'gem']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We initialized HashingVectorizer with tokenizer funciton and set the number of features to math(2**21).  Furthermore, we reinitialized a logistic regression classifier by setting the loss parameter of the SGDClassifier to ‘log’. The reason to choose a large number of features in HashingVectorizer is to reduce the chance of causing hash collisions while increasing the number of coefficients in the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.870\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What ‘dump’ method does is, it serialize the trained logistic regression model as well as ‘stop word’ set from NLTK library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(stop, \n",
    "            open(os.path.join(dest, 'stopwords.pkl'), 'wb'),\n",
    "            protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.plk'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraints.txt\n",
      "data\n",
      "docs\n",
      "download_data.ipynb\n",
      "gen_requirements_txt\n",
      "licence_research.ipynb\n",
      "licenser\n",
      "movieclassifier\n",
      "movie_data.csv\n",
      "requirements_all_py3_dev.txt\n",
      "requirements_all_py3.txt\n",
      "requirements_analytics_py3.in\n",
      "requirements.txt\n",
      "setup.cfg\n",
      "text_file.txt\n",
      "tox.ini\n",
      "vendor\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
